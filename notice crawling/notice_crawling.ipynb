{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9XImtRHuRb7774CUhjCt9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dae1jeong/SSU_25_NLP_project/blob/main/notice%20crawling/notice_crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "\n",
        "def crawl_post_content(url):\n",
        "    \"\"\"단일 게시글 URL에서 본문 텍스트, 이미지, 링크를 추출하여 반환\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # 게시글 전체 영역 지정\n",
        "        post_wrapper = soup.find('div', class_='bg-white p-4 mb-5')\n",
        "        if not post_wrapper:\n",
        "            return None\n",
        "\n",
        "        # --- 메타데이터 추출 (옵션) ---\n",
        "        # --- (생략: 메타데이터 추출 부분 - 이 함수에서는 반환하지 않고 목록 크롤러에서 사용) ---\n",
        "\n",
        "        # --- 본문 영역 지정 (HR 태그 이후) ---\n",
        "        # hr 태그를 찾고, 그 다음부터 본문이 시작되는 div를 찾습니다.\n",
        "        hr_tag = post_wrapper.find('hr', class_='mb-4 mt-4')\n",
        "\n",
        "        # hr_tag 바로 다음 형제(sibling) 요소를 본문 컨테이너로 간주\n",
        "        content_container = hr_tag.find_next_sibling('div') if hr_tag else None\n",
        "\n",
        "        if not content_container:\n",
        "            print(f\"경고: 본문 콘텐츠 영역을 찾을 수 없습니다. URL: {url}\")\n",
        "            return {'full_body_text': '본문 없음', 'image_urls': []}\n",
        "\n",
        "        # --- 본문 내용 추출 및 데이터 수집 ---\n",
        "        full_text_content = []\n",
        "        image_urls = []\n",
        "\n",
        "        # p, ol, ul, table 등 본문을 구성하는 모든 요소를 순회\n",
        "        for element in content_container.find_all(recursive=False):\n",
        "\n",
        "            # A. 테이블 처리\n",
        "            if element.name == 'table':\n",
        "                table_text = \"\\n[TABLE START] \"\n",
        "                headers = [th.get_text(strip=True) for th in element.find_all('th')]\n",
        "\n",
        "                for row in element.find_all('tr'):\n",
        "                    row_data = [td.get_text(strip=True).replace('\\n', ' ') for td in row.find_all('td')]\n",
        "                    if row_data:\n",
        "                        # 헤더와 데이터 연결하여 텍스트로 변환\n",
        "                        row_structure = \" | \".join(f\"{h}: {d}\" for h, d in zip(headers, row_data))\n",
        "                        table_text += row_structure + \" / \"\n",
        "\n",
        "                full_text_content.append(table_text + \" [TABLE END]\\n\")\n",
        "\n",
        "            # B. 순서 목록 (<ol>, <ul>) 처리\n",
        "            elif element.name in ['ol', 'ul']:\n",
        "                list_text = \"[LIST START] \"\n",
        "                for i, item in enumerate(element.find_all('li', recursive=False)):\n",
        "                    list_text += f\"({i+1}. {item.get_text(strip=True)}) \"\n",
        "                full_text_content.append(list_text + \" [LIST END]\\n\")\n",
        "\n",
        "            # C. 일반 단락/링크/이미지 (<p>, <a>, <img>) 처리\n",
        "            elif element.name in ['p', 'div']:\n",
        "                # 이미지 URL 추출\n",
        "                for img_tag in element.find_all('img'):\n",
        "                    img_src = img_tag.get('src')\n",
        "                    if img_src and img_src not in image_urls:\n",
        "                        image_urls.append(img_src)\n",
        "\n",
        "                # 순수 텍스트 추출 (링크, 이미지 태그 제거 후)\n",
        "                # element.get_text()는 자식 태그의 텍스트도 모두 가져오므로 여기서 추출\n",
        "                text = element.get_text(strip=True)\n",
        "                text = text.replace('\\n', ' ').replace('\\xa0', '').strip()\n",
        "                if text:\n",
        "                    # 텍스트 강조 처리 (선택 사항)\n",
        "                    text = re.sub(r'(\\w+)\\s*:', r'**\\1**:', text)\n",
        "                    full_text_content.append(text)\n",
        "\n",
        "\n",
        "        # 최종 텍스트 콘텐츠를 하나의 문자열로 결합\n",
        "        final_text = \"\\n\".join(full_text_content)\n",
        "        final_text = re.sub(r'\\n+', '\\n', final_text).strip() # 연속된 줄바꿈 제거\n",
        "\n",
        "        # 이 함수는 이 두 필드를 반환합니다.\n",
        "        return {\n",
        "            'full_body_text': final_text,\n",
        "            'image_urls': image_urls\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"상세 크롤링 오류 발생: {url}, 오류: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"상세 크롤링 중 알 수 없는 오류: {url}, 오류: {e}\")\n",
        "        return None\n",
        "\n",
        "# # 예시 사용법\n",
        "# # data = crawl_post_content(\"https://scatch.ssu.ac.kr/%EA%B3%B5%EC%A7%80%EC%82%AC%ED%95%AD/?slug=2025%ed%95%99%eb%85%84%eb%8f%84-2%ed%95%99%ea%b8%b0-sic-4%ec%bf%bc%ed%84%b0-%ec%a0%95%ea%b8%b0-%ea%b5%90%ed%99%98%ed%95%99%ec%83%9d-%ec%84%b8%eb%af%b8%eb%82%98-saf\")\n",
        "# # print(json.dumps(data, ensure_ascii=False, indent=4))"
      ],
      "metadata": {
        "id": "B0Jwix4J_x45"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime as dt\n",
        "from datetime import date\n",
        "import locale\n",
        "\n",
        "def convert_korean_date_to_iso(korean_date_str):\n",
        "    \"\"\"\n",
        "    'YYYY.MM.DD' 형식의 날짜 문자열을 ISO 8601 형식(YYYY-MM-DD)으로 변환합니다.\n",
        "    \"\"\"\n",
        "    if not korean_date_str:\n",
        "        return None\n",
        "\n",
        "    # 형식: YYYY.MM.DD 또는 YYYY-MM-DD 등으로 들어올 수 있음.\n",
        "    # 여기서는 '2025.11.04' 형식을 가정하고 파싱합니다.\n",
        "    try:\n",
        "        dt_object = dt.strptime(korean_date_str.strip().replace('/', '.').replace('-', '.'), '%Y.%m.%d')\n",
        "        return dt_object.strftime('%Y-%m-%d')\n",
        "    except ValueError:\n",
        "        # 실패 시 None 반환\n",
        "        return None\n",
        "\n",
        "# --- 사용 예시 ---\n",
        "# date_str_kor = '2025.11.04'\n",
        "# iso_date = convert_korean_date_to_iso(date_str_kor)\n",
        "# print(f\"원본: {date_str_kor} -> ISO: {iso_date}\") # 출력: 2025-11-04"
      ],
      "metadata": {
        "id": "VBtprDZ3HVGY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime as dt\n",
        "from datetime import date\n",
        "\n",
        "BASE_URL = \"https://scatch.ssu.ac.kr/%ea%b3%b5%ec%a7%80%ec%82%ac%ed%95%ad/\"\n",
        "TARGET_YEAR = 2025\n",
        "\n",
        "def fetch_notices_all_pages(base_url, max_pages):\n",
        "    \"\"\"\n",
        "    게시글 목록을 순회하며 2025년 이후의 항목만 필터링하여 메타데이터를 추출합니다.\n",
        "    \"\"\"\n",
        "    all_notices_metadata = []\n",
        "\n",
        "    # 날짜 필터링 기준 (2024년 12월 31일)\n",
        "    cutoff_date = date(TARGET_YEAR - 1, 12, 31)\n",
        "\n",
        "    for page_num in range(1, max_pages + 1):\n",
        "        # URL 경로 설정: '/page/N/' 형태\n",
        "        url = f\"{base_url}page/{page_num}/\" if page_num > 1 else base_url\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # 💡 정확한 선택자: 개별 게시글 항목을 나타내는 div\n",
        "            notice_items = soup.select('div.row.no-gutters.align-items-center')\n",
        "\n",
        "            if not notice_items:\n",
        "                print(f\"페이지 {page_num} ({url})에서 항목을 찾을 수 없어 종료합니다.\")\n",
        "                break\n",
        "\n",
        "            should_break = False\n",
        "\n",
        "            for item in notice_items:\n",
        "                # 1. 게시일 추출 및 필터링\n",
        "                date_element = item.select_one('.notice_col1 .h2')\n",
        "                raw_date = date_element.get_text(strip=True) if date_element else None\n",
        "\n",
        "                if raw_date:\n",
        "                    try:\n",
        "                        # 'YYYY.MM.DD' 형식을 datetime 객체로 파싱\n",
        "                        post_date = dt.strptime(raw_date, '%Y.%m.%d').date()\n",
        "\n",
        "                        # 2024년 이전 게시글이라면 순회 중단 플래그 설정\n",
        "                        if post_date <= cutoff_date:\n",
        "                            print(f\"2024년 이전 게시글 발견 ({post_date}). 목록 순회 중단.\")\n",
        "                            should_break = True\n",
        "                            break # item 순회 중단\n",
        "\n",
        "                    except ValueError:\n",
        "                        pass # 날짜 파싱 오류는 무시\n",
        "\n",
        "                # 2. 2025년 이후 항목에 대해서만 메타데이터 추출\n",
        "                try:\n",
        "                    status_element = item.select_one('.notice_col2 .tag')\n",
        "                    status = status_element.text.strip() if status_element else 'N/A'\n",
        "\n",
        "                    link_element = item.select_one('.notice_col3 a')\n",
        "                    link_href = link_element['href'].strip() if link_element else 'N/A'\n",
        "\n",
        "                    category_element = link_element.select_one('.label')\n",
        "                    category = category_element.text.strip() if category_element else 'N/A'\n",
        "\n",
        "                    title_span = link_element.select_one('span[class*=\"d-inline-b\"].m-pt-5')\n",
        "                    title = title_span.text.strip() if title_span else '제목 없음'\n",
        "\n",
        "                    all_notices_metadata.append({\n",
        "                        '작성일': raw_date,\n",
        "                        '진행상황': status,\n",
        "                        '카테고리': category,\n",
        "                        '제목': title,\n",
        "                        '링크': link_href,\n",
        "                    })\n",
        "\n",
        "                except Exception:\n",
        "                    continue # 항목 파싱 오류 무시\n",
        "\n",
        "            if should_break:\n",
        "                break # 전체 페이지 순회 중단\n",
        "\n",
        "            print(f\"페이지 {page_num} 크롤링 완료. 다음 페이지로 진행합니다.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"페이지 {page_num} ({url})를 가져오는 중 오류가 발생했습니다: {e}\")\n",
        "            break\n",
        "\n",
        "    return all_notices_metadata\n",
        "\n",
        "\n",
        "def crawl_and_save_rag_data(notices_metadata, filename=\"ssu_rag_data_2025.jsonl\"):\n",
        "    \"\"\"\n",
        "    수집된 목록 메타데이터를 순회하며 상세 크롤링을 수행하고 JSON Lines 파일로 저장합니다.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- RAG 상세 크롤링 및 JSON Lines 파일 생성 시작 (총 {len(notices_metadata)}개) ---\")\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for i, notice in enumerate(notices_metadata):\n",
        "            link = notice['링크']\n",
        "\n",
        "            # 1. 상세 크롤링 (본문 및 이미지 URL 획득)\n",
        "            detailed_data = crawl_post_content(link)\n",
        "\n",
        "            if not detailed_data:\n",
        "                continue\n",
        "\n",
        "            # 2. RAG 레코드 통합 및 포맷팅\n",
        "            rag_record = {\n",
        "                'source_url': link,\n",
        "                'post_title': notice['제목'],\n",
        "                'category': notice['카테고리'],\n",
        "                'status': notice['진행상황'],\n",
        "                'posted_date': convert_korean_date_to_iso(notice['작성일']), # ISO 변환\n",
        "                'full_body_text': detailed_data['full_body_text'],\n",
        "                'image_urls': detailed_data['image_urls']\n",
        "            }\n",
        "\n",
        "            # 3. JSON Lines 파일에 저장\n",
        "            f.write(json.dumps(rag_record, ensure_ascii=False) + '\\n')\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                 print(f\"✅ {i+1}번째 항목까지 JSON Lines 저장 완료.\")\n",
        "\n",
        "    print(f\"\\n🎉 RAG 데이터가 '{filename}' 파일에 성공적으로 저장되었습니다.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MAX_PAGES = 80\n",
        "\n",
        "    print(f\"--- 숭실대학교 공지사항 크롤링 시작 ({TARGET_YEAR}년 이후 필터링 및 페이지네이션) ---\")\n",
        "\n",
        "    metadata_results = fetch_notices_all_pages(BASE_URL, MAX_PAGES)\n",
        "\n",
        "    if metadata_results:\n",
        "        print(f\"\\n✅ 최종적으로 총 {len(metadata_results)}개의 ({TARGET_YEAR}년 이후) 공지사항 메타데이터를 수집했습니다.\")\n",
        "\n",
        "        # 2단계: 상세 크롤링 및 JSON Lines 저장\n",
        "        crawl_and_save_rag_data(metadata_results)\n",
        "\n",
        "    else:\n",
        "        print(\"공지사항을 찾지 못했거나 크롤링에 실패했습니다. URL과 CSS Selector를 다시 확인해 주세요.\")\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRdaugJYGGrB",
        "outputId": "29e40bd3-36ef-401e-fa2c-1a0eb06fe772"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 숭실대학교 공지사항 크롤링 시작 (2025년 이후 필터링 및 페이지네이션) ---\n",
            "페이지 1 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 2 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 3 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 4 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 5 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 6 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 7 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 8 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 9 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 10 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 11 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 12 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 13 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 14 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 15 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 16 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 17 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 18 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 19 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 20 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 21 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 22 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 23 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 24 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 25 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 26 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 27 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 28 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 29 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 30 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 31 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 32 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 33 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 34 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 35 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 36 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 37 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 38 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 39 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 40 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 41 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 42 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 43 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 44 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 45 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 46 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 47 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 48 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 49 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 50 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 51 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 52 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 53 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 54 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 55 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 56 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 57 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 58 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 59 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 60 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 61 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 62 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 63 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 64 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 65 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 66 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 67 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 68 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 69 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 70 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 71 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 72 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 73 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "페이지 74 크롤링 완료. 다음 페이지로 진행합니다.\n",
            "2024년 이전 게시글 발견 (2024-12-31). 목록 순회 중단.\n",
            "\n",
            "✅ 최종적으로 총 1111개의 (2025년 이후) 공지사항 메타데이터를 수집했습니다.\n",
            "\n",
            "--- RAG 상세 크롤링 및 JSON Lines 파일 생성 시작 (총 1111개) ---\n",
            "✅ 100번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 200번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 300번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 400번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 500번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 600번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 700번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 800번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 900번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 1000번째 항목까지 JSON Lines 저장 완료.\n",
            "✅ 1100번째 항목까지 JSON Lines 저장 완료.\n",
            "\n",
            "🎉 RAG 데이터가 'ssu_rag_data_2025.jsonl' 파일에 성공적으로 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def print_jsonl_details(file_path, num_records=5):\n",
        "    \"\"\"JSON Lines 파일에서 처음 N개의 레코드를 읽어 세부 내용을 출력합니다.\"\"\"\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"오류: 파일 '{file_path}'를 찾을 수 없습니다. 경로를 확인해 주세요.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- 📄 '{file_path}' 파일의 처음 {num_records}개 레코드 출력 ---\")\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= num_records:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # 1. 각 줄을 JSON 객체로 파싱\n",
        "                doc = json.loads(line)\n",
        "\n",
        "                # 2. 필수 필드를 포함하여 출력 형식 지정\n",
        "                print(f\"\\n[레코드 #{i+1}]\")\n",
        "                print(f\"  제목: {doc.get('post_title', 'N/A')}\")\n",
        "                print(f\"  URL: {doc.get('source_url', 'N/A')}\")\n",
        "                print(f\"  작성일: {doc.get('posted_date', 'N/A')}\")\n",
        "                print(f\"  상태: {doc.get('status', 'N/A')}\")\n",
        "                print(f\"  이미지 수: {len(doc.get('image_urls', []))}개\")\n",
        "\n",
        "                # 3. 본문 텍스트는 너무 길 수 있으므로 일부만 출력\n",
        "                body_text = doc.get('full_body_text', '본문 없음')\n",
        "                print(f\"  본문: {body_text}\")\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"경고: {i+1}번째 줄은 유효한 JSON 형식이 아닙니다. 건너뜁니다.\")\n",
        "\n",
        "# --- 실행 ---\n",
        "FILE_NAME = 'ssu_rag_data_2025.jsonl'\n",
        "print_jsonl_details(FILE_NAME, num_records=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpvNX_SrH7TH",
        "outputId": "0ff8fd53-e93d-4669-fa64-6edfa6cb0f65"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 📄 'ssu_rag_data_2025.jsonl' 파일의 처음 5개 레코드 출력 ---\n",
            "\n",
            "[레코드 #1]\n",
            "  제목: 2026년도 (재)전기공사공제조합장학회 제21기 장학생 선발 안내\n",
            "  URL: https://scatch.ssu.ac.kr/%ea%b3%b5%ec%a7%80%ec%82%ac%ed%95%ad/?f&category&paged=1&slug=2026%EB%85%84%EB%8F%84-%EC%9E%AC%EC%A0%84%EA%B8%B0%EA%B3%B5%EC%82%AC%EA%B3%B5%EC%A0%9C%EC%A1%B0%ED%95%A9%EC%9E%A5%ED%95%99%ED%9A%8C-%EC%A0%9C21%EA%B8%B0-%EC%9E%A5%ED%95%99%EC%83%9D-%EC%84%A0&keyword\n",
            "  작성일: 2025-11-04\n",
            "  상태: 진행\n",
            "  이미지 수: 1개\n",
            "  본문: \n",
            "\n",
            "[레코드 #2]\n",
            "  제목: [중앙도서관] ‘천선란’ 작가 저자강연회 참가자 모집(~11/21)\n",
            "  URL: https://scatch.ssu.ac.kr/%ea%b3%b5%ec%a7%80%ec%82%ac%ed%95%ad/?f&category&paged=1&slug=%EC%A4%91%EC%95%99%EB%8F%84%EC%84%9C%EA%B4%80-%EC%B2%9C%EC%84%A0%EB%9E%80-%EC%9E%91%EA%B0%80-%EC%A0%80%EC%9E%90%EA%B0%95%EC%97%B0%ED%9A%8C-%EC%B0%B8%EA%B0%80%EC%9E%90-%EB%AA%A8&keyword\n",
            "  작성일: 2025-11-04\n",
            "  상태: 진행\n",
            "  이미지 수: 2개\n",
            "  본문: 슈패스 **신청기간**: 11/3(월) ~ 21(금) ‘저자강연회’ 검색 후 신청*선착순마감\n",
            "안녕하세요. 중앙도서관입니다.\n",
            "2025학년도 2학기 저자강연회에는\n",
            "SF소설을 통해 따뜻한 상상력을 전하는천선란작가님을 강연자로 모셨습니다.\n",
            "작가님이 들려주는 글과 SF의 세계로 여러분을 초대합니다.\n",
            "[ 저자강연회 참가 안내 ]\n",
            "1. **일시**: 2025.11.26(수) **18**:00~**20**:00(강연, 사전Q&A+현장Q&A, 기념촬영)\n",
            "※ 원활한 참석을 위해 행사시작 10분전 도착부탁드립니다\n",
            "※ 현장 상황에 따라 강연시간이 늘어날 수 있습니다.\n",
            "강연회 이후 일정이 있는 분들은 뒷문을 통해 퇴실가능하니 참고바랍니다.\n",
            "2. **장소**: 벤처관 309호\n",
            "3. **신청대상**: 본교 재학생(재학/휴학) 120명 모집\n",
            "4. **신청방법**: 슈패스를 통해 신청\n",
            "5. **다과**: 샌드위치제공\n",
            "6. 사전Q&**A**: 저자강연회 중 작가님께 궁금한 내용이나 다른 학우들과 함께 듣고 싶은 내용을 신청시 함께 작성해주세요!\n",
            "7. 미참석시 **패널티**:블랙리스트(3개월) 등록 될 예정입니다. 참석이 어려울 경우 신청기간내 취소하셔서 대기자분들이 등록할 수 있도록 확인부탁드립니다.\n",
            "8. **담당**:중앙도서관 이준학(02-820-0739 / ljhak@ssu.ac.kr)\n",
            "※사전Q&A 현장 사진(2025-1정세랑 작가강연회)\n",
            "\n",
            "[레코드 #3]\n",
            "  제목: 2025학년도 2학기 SIC 4쿼터 정기 교환학생 세미나 (SAF)\n",
            "  URL: https://scatch.ssu.ac.kr/%ea%b3%b5%ec%a7%80%ec%82%ac%ed%95%ad/?f&category&paged=1&slug=2025%ED%95%99%EB%85%84%EB%8F%84-2%ED%95%99%EA%B8%B0-sic-4%EC%BF%BC%ED%84%B0-%EC%A0%95%EA%B8%B0-%EA%B5%90%ED%99%98%ED%95%99%EC%83%9D-%EC%84%B8%EB%AF%B8%EB%82%98-saf&keyword\n",
            "  작성일: 2025-11-04\n",
            "  상태: 진행\n",
            "  이미지 수: 1개\n",
            "  본문: 안녕하세요, SIC(Soongsil International Community, 숭실 국제 커뮤니티)입니다.\n",
            "SIC 4쿼터 교환학생 세미나에 여러분을 초대합니다!\n",
            "SIC 교환학생 세미나는 교환학생 프로그램을 통해 해외 대학에 파견되었던 교환학생 선배님들의 경험과 꿀팁을 듣는 숭실대 유일의 교환학생 관련 행사입니다.\n",
            "교환학생 준비부터 다채로운 해외 경험까지 선배님들의 교환학생 후기가 궁금한 모든 숭실대 학우분들께 참여를 추천드립니다.\n",
            "또한 이번 세미나에서는 SAF에서 직접 방문하여 학우분들께 교환학생 파견 전략 및 유용한 팁에 대해 설명하는 시간도 예정되어 있습니다.\n",
            "[LIST START] (1. 행사개요가. 세미나 일정 : 11월 20일 목요일 18:30-20:30나. 세미나 장소 : 숭실대학교 벤처중소기업센터 309호다. 세미나 진행 : 최수인 (SIC Management)라. 세미나 연사)  [LIST END]\n",
            "[TABLE START]  /  /  /  /  [TABLE END]\n",
            "2. 신청 **링크https**://buly.kr/A46S0DT\n",
            "3. 문의처\n",
            "가. **이메일**: ssuintlc@gmail.com\n",
            "나. 네이버 **카페**: **https**://cafe.naver.com/ssuabroad\n",
            "다. **인스타그램**: @ssu_sic_\n",
            "——-\n",
            "안녕하세요, 우리는 SIC (Soongsil International Community, 숭실 국제 커뮤니티)입니다.\n",
            "SIC는 2020년 숭실대학교 선배님들의 자발적인 글로벌 커리어 재능기부 활동으로 시작하여, 현재 교환학생, 해외 대학원 진학, 해외/외국계 기업 취업에 경험/관심있는 숭실 학우 1500명 이상이 참여하고 있는 커뮤니티입니다. SIC 네이버 카페에는 세계 각지 다양한 분야에서 종사하는 선배님 150명 이상이 가입해 계십니다.\n",
            "SIC는 시작을 만들어 주신 선배님들의 선의를 이어, 숭실 학우분들의 글로벌 열망이 현실이 되도록 돕기 위해 국제처 비교과로 편입되어 더 폭넓고 다양한 활동을 이어가고 있습니다.\n",
            "SIC가 걸어온 길은 아래 링크에서 확인하실 수 있습니다\n",
            "SIC **History**:**https**://sic-history.notion.site/\n",
            "붙임 SIC 2025-2학기 4쿼터 정기 교환학생 세미나 (SAF) 포스터 1부. 끝.\n",
            "2025.11.04.\n",
            "국제처장\n",
            "\n",
            "[레코드 #4]\n",
            "  제목: 2025년 우양재단 동행 장학생 모집 안내\n",
            "  URL: https://scatch.ssu.ac.kr/%ea%b3%b5%ec%a7%80%ec%82%ac%ed%95%ad/?f&category&paged=1&slug=%EC%9A%B0%EC%96%91%EC%9E%AC%EB%8B%A8-2025%EB%85%84-%EB%8F%99%ED%96%89-%EC%9E%A5%ED%95%99%EC%83%9D-%EB%AA%A8%EC%A7%91-%EC%95%88%EB%82%B4&keyword\n",
            "  작성일: 2025-11-03\n",
            "  상태: 진행\n",
            "  이미지 수: 1개\n",
            "  본문: [LIST START] (1. 붙임2-2025년-동행-장학-지원분야별-신청서서식.zip)  [LIST END]\n",
            "\n",
            "[레코드 #5]\n",
            "  제목: 2026-2학기 FHV 교환학생(Ernst Mach Grant장학금) 참가자 모집 안내\n",
            "  URL: https://scatch.ssu.ac.kr/%ea%b3%b5%ec%a7%80%ec%82%ac%ed%95%ad/?f&category&paged=1&slug=2026-2%ED%95%99%EA%B8%B0-fhv-%EA%B5%90%ED%99%98%ED%95%99%EC%83%9Dernst-mach-grant%EC%9E%A5%ED%95%99%EA%B8%88-%EC%B0%B8%EA%B0%80%EC%9E%90-%EB%AA%A8%EC%A7%91-%EC%95%88%EB%82%B4&keyword\n",
            "  작성일: 2025-11-03\n",
            "  상태: 진행\n",
            "  이미지 수: 0개\n",
            "  본문: 본교 자매대학인 오스트리아 FHV (Vorarlberg University of Applied Sciences) 대학의 교환학생(Ernst Mach Grant장학금) 참가자를 다음과 같이 모집하니 관심있는 학생들의 많은 참여 바랍니다.\n",
            "* 상대교에 장학생 지원 후 상대교에서도 평가를 진행합니다. 때문에 최종 선발 안될 수도 있는 점 참고하시기 바랍니다.\n",
            "[LIST START] (1. 프로그램 개요(관련링크:https://bit.ly/2MjAZBL))  [LIST END]\n",
            "가. **파견기간**: 1학기 (2026-**2학기**: 2026년 8월 ~ 2026년 12월)\n",
            "나. 프로그램 **진행절차**: 링크 참조 (**https**://bit.ly/3cazQat)\n",
            "다. **장학금액**: 월 1,300유로 지급\n",
            "라. 상대교 제공 자료\n",
            "1) 상대교 **사이트https**://www.fhv.at/en\n",
            "2) 장학금 요약 **정보https**://buly.kr/G3ECzCE\n",
            "3) 장학금 구체 **정보https**://buly.kr/BeLGSoL\n",
            "4)수강 가능 학과\n",
            "Autumn/Winter Semester (BACHELOR’S DEGREES)\n",
            "[LIST START] (1. Computer Science – Software Engineering (semester 5) :https://buly.kr/58TEAia) (2. Electronics and Information Technology Dual (semester 5) :https://buly.kr/5UIk8LZ) (3. Environment and Engineering (semester 5) :https://buly.kr/B7azWRv) (4. International Business (semester 5) :https://buly.kr/2fecRHs) (5. Mechatronics (semester 5) :https://buly.kr/6XnI1Fv)  [LIST END]\n",
            "마. 기타 참고사항\n",
            "1) 교환학생 **학점인정절차**:**https**://study.ssu.ac.kr/program/student4.do2) 자매대학 **소개**:**https**://study.ssu.ac.kr/community/uni_list.do\n",
            "3) 해외교육체험기(파견 다녀온 학생 보고서 확인 가능):**https**://study.ssu.ac.kr/community/exp_list.do\n",
            "4) **FAQ**:**https**://study.ssu.ac.kr/program/student5.do\n",
            "[LIST START] (1. 지원절차)  [LIST END]\n",
            "가. 지원자격\n",
            "[TABLE START]  /  /  /  /  /  /  /  /  /  [TABLE END]\n",
            "나. 지원 및 선발 절차\n",
            "[TABLE START]  /  /  /  /  /  [TABLE END]\n",
            "[LIST START] (1. 유의사항)  [LIST END]\n",
            "가. 지원 관련 유의점\n",
            "1) 본인이 구사하는 외국어를 모국어로 하지 않는 국가의 학교 지원 시, 수강 가능 과목을 사전에 확인 후 지원\n",
            "2) 파견 희망하는 대학에 본인의 전공이 지원 가능한지 조사 후 지원해야 하며, 특히 예술창작학부 학생의 경우 전공 수업 수강이 제한적일 수 있음\n",
            "※ 상대교 홈페이지 및 본교 홈페이지의 자매대학 소개, 해외교육체험기 등을 참고할 것\n",
            "3) 유효기간이 명시되지 않은 어학 성적도 2년 이내의 성적만 제출 가능\n",
            "4) 최소 지원 자격을 갖추었더라도 서류 및 면접 결과 적절한 수준에 미치지 못할 경우 선발하지 않음\n",
            "5) 제출한 서류의 위조, 대필, 대리 작성이 확인되는 경우 지원자격 박탈 또는 합격취소 처리됨\n",
            "6) 파견 가능 학과 및 파견 여부는 상대교 사정에 따라 사전 고지 없이 변경 될 수 있음\n",
            "나. 서류 관련 유의점\n",
            "1) [붙임2] 지원서 파일의 경우, 좌측 상단의 보기-문서 편집 버튼을 눌러야 작성이 가능하며, 회색 부분에 입력할 수 있음. 회색 부분은 글씨 크기 등의 수정이 불가능하게 되어 있으며, 자기소개서 및 학업계획서는 자유로운 양식으로 제출하여도 됨.\n",
            "2) 어학 성적 기준 관련하여, 국제팀 홈페이지-커뮤니티-자매대학소개에 있는 자료 중 [국제팀] 대륙 국가 학교명 (년도) pdf 파일의 경우 과거의 게시물로, 최신 어학 성적 기준은 [붙임5]파일을 참고\n",
            "3) 어떠한 지원 방식이든(일반교환/방문/SHP/지역전문가/방문장학) 서류는 단 1개의 pdf파일로 ‘병합해서’ 제출(지원서 -어학성적 별개로 제출하는 게 아님 파일 별로 전송 x , pdf editor 사용 권장)\n",
            "4) 장학금 중복지원 가능하나, 여러 개 합격 시 상위 장학금으로 자동배정 됨. 장학금 지원자는 일반교환학생 면접 (1차) & 장학금 면접 (2차), 총 2번의 면접 진행됨. 장학금 선발 과정에서 불합격하는 경우에는 일반/방문 교환학생 전형으로 자동 전환됨.\n",
            "5) (중요) 서류 제출 기한에 서류를 검토 후 누락된 것이 있을 경우 여러분들께 전화/메일/문자메시지 등의 수단으로 연락 예정 02-820-0757 전화번호는 국제팀 번호이니, 꼭 수신바람, 더불어 study@ssu.ac.kr 메일이 스팸함에 있을 수 있으니 스팸함 또한 확인 필요\n",
            "6) (중요 QnA) 지역전문가양성/SHP를 지원하는 동시에 그냥 일반/방문 교환학생 또한 지원하고 싶은데 어떻게 해야 하나요?\n",
            ": 2개를 구분하실 필요 없습니다. 일반/방문 교환학생에 혜택을 주는 것이 지역전문가양성/SHP 장학 프로그램이기 때문에, 학생 분께서는 1~3지망을 2개를 제출하시는 것이 아니라, 예를 들어 (1지망) SHP – 일반 – OO 학교 / (2지망) SHP – 방문 – XX 학교 / (3지망) SHP – 일반 – TT 학교 이런 식으로 1개만 작성하시면 됩니다.\n",
            "7) 성적증명서는 학교 내 증명서발급기 혹은 숭실대 홈페이지(ssu.ac.kr) – 학사 – 증명서 발급 – 웹민원센터에서 로그인 후 증명서 –산술평균표기로 발급후 출력\n",
            "[LIST START] (1. 문의처: 국제팀 (신양관 203호, 02-820-0757,study@ssu.ac.kr,https://study.ssu.ac.kr/main/main.do))  [LIST END]\n",
            "※모집정원,면접형식,과거합격자 성적 등에 관한 문의는 답변해 드릴 수 없음을 양해해 주시기 바랍니다.\n",
            "붙임 1. 교환학생 프로그램 지원서 1부.\n",
            "[LIST START] (1. 해외파견 서약서 및 개인정보제공활용 동의서 1부.) (2. 지원 서류 샘플 모음 1부.) (3. 파견 교환학생 가이드 1부.) (4. 공인어학성적 기준표 1부.  끝.)  [LIST END]\n",
            "2025.11. 03.국 제 처 장\n",
            "[LIST START] (1. 붙임1-교환학생-프로그램-지원서.docx) (2. 붙임2-해외파견-서약서-및-개인정보제공활용-동의서.pdf) (3. 붙임3-지원-서류-샘플-모음.pdf) (4. 붙임4-파견-교환학생-가이드.pdf) (5. 붙임5-공인어학성적-기준표.pdf)  [LIST END]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ak-12vbvTVFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
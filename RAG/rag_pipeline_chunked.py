# ==============================================================================
# SSU_25_NLP_project - RAG/rag_pipeline_chunked.py
#
# [ê°œìš”]
# ì´ íŒŒì¼ì€ RAG ì±—ë´‡ì˜ ìµœì¢… ì™„ì„± ë²„ì „ìœ¼ë¡œ, í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ê¸°ìˆ ì¸
# 'í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰'ì„ êµ¬í˜„í•œ ë©”ì¸ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
# main.pyì—ì„œ ì´ í´ë˜ìŠ¤ë¥¼ ë¡œë“œí•˜ì—¬ ì„œë¹„ìŠ¤ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
#
# [ì£¼ìš” íŠ¹ì§•]
# 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: BM25 ê²€ìƒ‰ê³¼ ë²¡í„° ê²€ìƒ‰ì„ ë™ì‹œì— ìˆ˜í–‰.
# 2. RRF ìœµí•©: ë‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ RRF(Reciprocal Rank Fusion) ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í†µí•©í•˜ì—¬ ìµœì ì˜ ë¬¸ì„œ í™•ë³´.
# 3. NLU í•„í„°ë§: NLUê°€ íŒŒì•…í•œ ì˜ë„(Intent) ë° ê°œì²´ëª…(Slot)ì„ ì‚¬ìš©í•´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•„í„°ë§.
# 4. ì‹¤ì‹œê°„ ê¸°ëŠ¥: í•™ì‹ ê´€ë ¨ ì§ˆë¬¸ì€ RAGë¥¼ ê±´ë„ˆë›°ê³  ì‹¤ì‹œê°„ ì›¹ ìŠ¤í¬ë˜í•‘ì„ í†µí•´ ì¦‰ì‹œ ì‘ë‹µ.
#
# [ì‚¬ìš© DB]
# - BM25DBRetriever: bm25_tokens.db (BM25 ì „ìš©)
# - Vector Search: chroma_db (ë²¡í„° ì „ìš©)
# ==============================================================================




from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict, Optional, Callable
import sqlite3
import os
from datetime import datetime
import json
import re  # ì •ê·œì‹
import time

import collections
import collections.abc
# bs4ê°€ Python 3.12ì—ì„œ collections.Callableì„ ì°¸ì¡°í•´ì„œ ë‚˜ëŠ” ì˜¤ë¥˜ ë°©ì§€ìš© íŒ¨ì¹˜
if not hasattr(collections, "Callable"):
    collections.Callable = collections.abc.Callable  # type: ignore[attr-defined]

import numpy as np
from rank_bm25 import BM25Okapi
from openai import OpenAI
from dotenv import load_dotenv

import requests
from bs4 import BeautifulSoup

# ğŸ”¹ ì¶”ê°€: Chroma + SBERT
import chromadb
from sentence_transformers import SentenceTransformer

# ------------------------------------------------------------
# í™˜ê²½ ë³€ìˆ˜ (í† í¬ë‚˜ì´ì € ê²½ê³  ì¤„ì´ê¸°ìš©, ì„ íƒ)
# ------------------------------------------------------------
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("OMP_NUM_THREADS", "1")

# ------------------------------------------------------------------
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ / DB / HF ìºì‹œ / .env ì„¤ì •
# ------------------------------------------------------------------
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DB_PATH = os.path.join(ROOT_DIR, "bm25_tokens.db")          # BM25 í† í° DB
CHROMA_PATH = os.path.join(ROOT_DIR, "chroma_db")           # Chroma ë²¡í„° DB
CHROMA_COLLECTION_NAME = "ssu_knowledge_base"
EMBEDDING_MODEL_NAME = "jhgan/ko-sbert-nli"

HF_CACHE_DIR = os.path.join(ROOT_DIR, "hf_cache")
os.makedirs(HF_CACHE_DIR, exist_ok=True)

os.environ["HF_HOME"] = HF_CACHE_DIR
os.environ["TRANSFORMERS_CACHE"] = HF_CACHE_DIR
os.environ["SENTENCE_TRANSFORMERS_HOME"] = HF_CACHE_DIR

print("[RAG] Using DB_PATH =", DB_PATH)
print("[RAG] Using HF cache dir =", HF_CACHE_DIR)
print("[RAG] Using CHROMA_PATH =", CHROMA_PATH)

env_path = os.path.join(ROOT_DIR, ".env")
print(f"[RAG] Loading .env from: {env_path}")

if os.path.exists(env_path):
    load_dotenv(env_path)
    print("[RAG] .env íŒŒì¼ ë¡œë“œ ì„±ê³µ")
else:
    print("[RAG] ğŸš¨ ê²½ê³ : .env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

# ==========================================================
# 0. í•™ì‹ ê´€ë ¨ ìŠ¤í¬ë˜í•‘ ìœ í‹¸ (soongguri / ê¸°ìˆ™ì‚¬ ì‹ë‹¹)
# ==========================================================

SOONGGURI_URL = "https://soongguri.com/m/"
DORM_FOOD_URL = (
    "https://ssudorm.ssu.ac.kr:444/"
    "SShostel/mall_main.php?viewform=B0001_foodboard_list&board_no=1"
)

# soongguriê°€ ëª¨ë°”ì¼ ë¸Œë¼ìš°ì €ë¼ê³  ë¯¿ë„ë¡ í—¤ë” ì„¸íŒ…
SOONGGURI_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}


def _clean_line(text: str) -> str:
    """ê³µë°± ì •ë¦¬ + ì“¸ëª¨ì—†ëŠ” ê¸°í˜¸ ì œê±°ìš© ìœ í‹¸."""
    if not text:
        return ""
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def _normalize_sdt(date_str: str | None) -> tuple[str, str]:
    """
    date_str:
      - None        â†’ ì˜¤ëŠ˜ ë‚ ì§œ
      - '20251125'  â†’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
      - '2025-11-25' â†’ '-' ì œê±° í›„ ì‚¬ìš©
    return: (sdt, pretty_label)
    """
    if not date_str:
        dt = datetime.now()
        sdt = dt.strftime("%Y%m%d")
        label = dt.strftime("%Y-%m-%d")
        return sdt, label

    ds = date_str.strip()

    # 2025-11-25 í˜•ì‹
    if len(ds) == 10 and ds[4] == "-" and ds[7] == "-":
        try:
            dt = datetime.strptime(ds, "%Y-%m-%d")
            return dt.strftime("%Y%m%d"), dt.strftime("%Y-%m-%d")
        except ValueError:
            pass

    # 20251125 í˜•ì‹
    if len(ds) == 8 and ds.isdigit():
        try:
            dt = datetime.strptime(ds, "%Y%m%d")
            return ds, dt.strftime("%Y-%m-%d")
        except ValueError:
            pass

    # ì´ìƒí•˜ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¡œ fallback
    dt = datetime.now()
    return dt.strftime("%Y%m%d"), dt.strftime("%Y-%m-%d")


def fetch_soongguri_menu(date_str: str | None = None, rcd: str = "1") -> str:
    """
    soongguri AJAX ì—”ë“œí¬ì¸íŠ¸(/m/m_req/m_menu.php)ì—ì„œ
    ì£¼ì–´ì§„ ë‚ ì§œ(date_str)ì™€ ì‹ë‹¹ ì½”ë“œ(rcd)ì˜ ë©”ë‰´ HTMLì„ ì§ì ‘ ê°€ì ¸ì™€ì„œ íŒŒì‹±.
    """
    sdt, label = _normalize_sdt(date_str)

    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    try:
        session = requests.Session()
        session.headers.update(SOONGGURI_HEADERS)

        # ì‹¤ì œ AJAX ë©”ë‰´ ë°ì´í„° ìš”ì²­ (m_menu.php)
        params = {"rcd": rcd, "sdt": sdt}
        resp = session.get(
            SOONGGURI_URL + "m_req/m_menu.php",
            params=params,
            timeout=10,
            verify=False,
        )
    except Exception as e:
        return (
            "[ìƒí˜‘ ì‹ë‹¹ ë©”ë‰´]\n"
            "soongguri ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì§€ ëª»í–ˆì–´ìš”.\n"
            f"(ì—ëŸ¬: {e})\n"
            "â†’ ì§ì ‘ í™•ì¸: https://soongguri.com/m/"
        )

    if resp.status_code != 200 or len(resp.text.strip()) < 50:
        return (
            f"[ìƒí˜‘ ì‹ë‹¹ ë©”ë‰´ - {label}]\n"
            "í˜„ì¬ soongguriì—ì„œ í•™ì‹ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”.\n"
            "â†’ ì§ì ‘ í™•ì¸: https://soongguri.com/m/"
        )

    soup = BeautifulSoup(resp.text, "html.parser")
    table = soup.find("table")

    if not table:
        return (
            f"[ìƒí˜‘ ì‹ë‹¹ ë©”ë‰´ - {label}]\n"
            "ì‹ë‹¨ í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”.\n"
            "â†’ ì§ì ‘ í™•ì¸: https://soongguri.com/m/"
        )

    menus: list[str] = []

    for tr in table.find_all("tr"):
        name_td = tr.find("td", class_="menu_nm")
        list_td = tr.find("td", class_="menu_list")
        if not (name_td and list_td):
            continue

        meal_name = name_td.get_text(strip=True)  # ì˜ˆ: ì¤‘ì‹1, ì„ì‹1...

        # ì½”ë„ˆëª… [ëšë°°ê¸°ì½”ë„ˆ] ë“±
        corner = ""
        for tag in list_td.find_all(["font", "b", "span"]):
            txt = tag.get_text(strip=True)
            m = re.search(r"\[[^\]]+\]", txt)
            if m:
                corner = m.group(0)
                break

        # ë©”ì¸ ë©”ë‰´ (â˜… í‘œì‹œ)
        main_dish = ""
        for tag in list_td.find_all(["font", "b", "span"]):
            txt = tag.get_text(" ", strip=True)
            if "â˜…" in txt:
                main_dish = txt.replace("â˜…", "").strip()
                break

        # ë°˜ì°¬ í›„ë³´ë“¤
        side_dishes: list[str] = []
        for li in list_td.select("ul.mean_list li, ul.mean_list td, ul.mean_list .xl65"):
            t = _clean_line(li.get_text(strip=True))
            if not t:
                continue
            if "ì•ŒëŸ¬ì§€ìœ ë°œì‹í’ˆ" in t or "ì›ì‚°ì§€" in t:
                continue
            if all(ord(ch) < 128 for ch in t):  # ì „ë¶€ ASCII(ì˜ë¬¸)ë©´ ìŠ¤í‚µ
                continue
            if t not in side_dishes:
                side_dishes.append(t)

        line = meal_name
        if corner:
            line += f" {corner}"
        if main_dish:
            line += f" - {main_dish}"
        if side_dishes:
            line += "\n  Â· " + "\n  Â· ".join(side_dishes)

        menus.append(line)

    if not menus:
        return (
            f"[ìƒí˜‘ ì‹ë‹¹ ë©”ë‰´ - {label}]\n"
            "ë©”ë‰´ íŒŒì‹± ì‹¤íŒ¨ (í•­ëª© ì—†ìŒ)\n"
            "â†’ https://soongguri.com/m/ ì—ì„œ ì§ì ‘ í™•ì¸í•´ ì£¼ì„¸ìš”."
        )

    return f"[ìƒí˜‘ ì‹ë‹¹ ë©”ë‰´ - {label}]\n" + "\n\n".join(menus)


def fetch_dorm_menu() -> str:
    """
    ìˆ­ì‹¤ëŒ€ ê¸°ìˆ™ì‚¬ ì‹ë‹¹ ì£¼ê°„ ì‹ë‹¨í‘œ(boxstyle02 í…Œì´ë¸”)ì—ì„œ 'ì˜¤ëŠ˜ ë‚ ì§œ'ì— í•´ë‹¹í•˜ëŠ”
    ì¡°ì‹/ì¤‘ì‹/ì„ì‹ ë©”ë‰´ë¥¼ íŒŒì‹±í•œë‹¤.
    """
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    try:
        resp = requests.get(DORM_FOOD_URL, timeout=10, verify=False)
        resp.raise_for_status()
    except Exception as e:
        return f"[í•™ì‹] ê¸°ìˆ™ì‚¬ ì‹ë‹¹ ì‚¬ì´íŠ¸ ì ‘ì† ì‹¤íŒ¨: {e}"

    soup = BeautifulSoup(resp.text, "html.parser")

    table = soup.find("table", class_="boxstyle02")
    if not table:
        return "[í•™ì‹] ê¸°ìˆ™ì‚¬ ì‹ë‹¹ ë©”ë‰´ í…Œì´ë¸”(boxstyle02)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    today = datetime.now().strftime("%Y-%m-%d")
    target_row = None

    # ì²« ë²ˆì§¸ trëŠ” í—¤ë”, ê·¸ ë‹¤ìŒë¶€í„° ì‹¤ì œ ë‚ ì§œ í–‰
    for tr in table.find_all("tr")[1:]:
        th = tr.find("th")
        if not th:
            continue
        text = th.get_text(strip=True)  # ì˜ˆ: "2025-11-21 (ê¸ˆ)"
        if today in text:
            target_row = tr
            break

    # ì˜¤ëŠ˜ ë‚ ì§œê°€ ì—†ìœ¼ë©´, ì£¼ê°„ ë©”ë‰´ ìš”ì•½ ë°˜í™˜
    if target_row is None:
        rows_text: list[str] = []
        for tr in table.find_all("tr")[1:]:
            th = tr.find("th")
            if not th:
                continue
            date_text = th.get_text(strip=True)
            tds = tr.find_all("td")
            if len(tds) < 3:
                continue
            breakfast = tds[0].get_text("\n", strip=True)
            lunch = tds[1].get_text("\n", strip=True)
            dinner = tds[2].get_text("\n", strip=True)
            row_str = (
                f"{date_text}\n"
                f"  Â· ì¡°ì‹: {breakfast}\n"
                f"  Â· ì¤‘ì‹: {lunch}\n"
                f"  Â· ì„ì‹: {dinner}"
            )
            rows_text.append(row_str)

        if not rows_text:
            return "[í•™ì‹] ê¸°ìˆ™ì‚¬ ì‹ë‹¹ ì£¼ê°„ ë©”ë‰´ë¥¼ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        return "[ê¸°ìˆ™ì‚¬ ì‹ë‹¹ ì£¼ê°„ ë©”ë‰´]\n\n" + "\n\n".join(rows_text)

    # ì˜¤ëŠ˜ ë‚ ì§œ í–‰ì„ ì°¾ì€ ê²½ìš°
    tds = target_row.find_all("td")
    breakfast = tds[0].get_text("\n", strip=True) if len(tds) >= 1 else ""
    lunch = tds[1].get_text("\n", strip=True) if len(tds) >= 2 else ""
    dinner = tds[2].get_text("\n", strip=True) if len(tds) >= 3 else ""
    both = tds[3].get_text("\n", strip=True) if len(tds) >= 4 else ""

    date_label = target_row.find("th").get_text(strip=True)

    lines = [
        f"[ê¸°ìˆ™ì‚¬ ì‹ë‹¹ ë©”ë‰´ - {date_label}]",
        f"Â· ì¡°ì‹: {breakfast or 'ë¯¸ìš´ì˜'}",
        f"Â· ì¤‘ì‹: {lunch or 'ë¯¸ë“±ë¡'}",
        f"Â· ì„ì‹: {dinner or 'ë¯¸ë“±ë¡'}",
    ]
    if both:
        lines.append(f"Â· ì¤‘Â·ì„ì‹: {both}")

    return "\n".join(lines)


def build_meal_context() -> str:
    """
    soongguri(ìƒí˜‘) + ê¸°ìˆ™ì‚¬ ì‹ë‹¹ ë©”ë‰´ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨.
    intent == "í•™ì‹_ê²€ìƒ‰"ì¼ ë•Œ ì´ ë¬¸ìì—´ì„ LLMì— ë„˜ê¸´ë‹¤.
    """
    soongguri_text = fetch_soongguri_menu()
    dorm_text = fetch_dorm_menu()

    context_parts = [
        "ë‹¤ìŒì€ ìˆ­ì‹¤ëŒ€í•™êµ ì˜¤ëŠ˜ì˜ í•™ì‹ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤.",
        "",
        soongguri_text,
        "",
        dorm_text,
    ]
    return "\n".join(context_parts)

# ======================== Evalì„ ìœ„í•œ ì½”ë“œ ====================
# ragas, Recall@K ë“± ë‹¤ë¥¸ í‰ê°€ë¥¼ ìœ„í•œ ë¦¬í„´ íƒ€ì… ì •ì˜
@dataclass
class EvaluationResult:
    """RAG ì‹œìŠ¤í…œ í‰ê°€ì— í•„ìš”í•œ ëª¨ë“  ê²°ê³¼ë¥¼ ë‹´ëŠ” êµ¬ì¡°"""
    query: str                       
    model_answer: str                
    retrieved_chunks: List['ChunkDocument'] 
    context_texts: List[str]         
    is_rag_flow: bool
    latency_seconds: float = 0.0 


# ==============================================
# 1. BM25 / RAG íŒŒíŠ¸
# ==============================================

@dataclass
class ChunkDocument:
    id: str
    text: str
    meta: Dict
    tokens: List[str]


def load_chunks_from_db(db_path: str = DB_PATH) -> List[ChunkDocument]:
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    cur.execute("SELECT id, text, metadata, tokens FROM chunks")
    rows = cur.fetchall()
    conn.close()

    docs: List[ChunkDocument] = []
    for id_, text, metadata_json, tokens_str in rows:
        meta = json.loads(metadata_json)
        tokens = tokens_str.split()
        docs.append(ChunkDocument(id=id_, text=text, meta=meta, tokens=tokens))
    print(f"[DB] ì´ {len(docs)}ê°œì˜ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    return docs


class BM25DBRetriever:
    def __init__(self, chunk_docs: List[ChunkDocument]):
        self.docs = chunk_docs
        self.corpus_tokens = [d.tokens for d in chunk_docs]  # DBì— ìˆëŠ” í† í° ì‚¬ìš©
        self.bm25 = BM25Okapi(self.corpus_tokens)
        print(f"[BM25] DB í† í°ìœ¼ë¡œ BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

    def search(self, query: str, top_k: int = 30) -> List[ChunkDocument]:
        query_tokens = query.strip().split()
        scores = self.bm25.get_scores(query_tokens)
        ranked_indices = np.argsort(-scores)[:top_k]
        return [self.docs[i] for i in ranked_indices]


# (ê¸°ì¡´ VectorRerankerëŠ” BM25 ìƒìœ„ kë§Œ ìë¥´ëŠ” ìš©ë„ë¼ì„œ, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë„ì… í›„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
# í•„ìš”í•˜ë©´ ë‚¨ê²¨ë‘ê³ , ì‹¤ì œ í˜¸ì¶œì€ í•˜ì§€ ì•ŠëŠ”ë‹¤.)
class VectorReranker:
    """
    âš ï¸ (í˜„ì¬ ë¯¸ì‚¬ìš©)
    ì„¸ê·¸í´íŠ¸ ë°©ì§€ë¥¼ ìœ„í•´ sentence_transformers ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ 
    BM25 ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ìƒìœ„ top_kë§Œ ì˜ë¼ì„œ ë°˜í™˜í•˜ëŠ” ë‹¨ìˆœí•œ reranker.
    """

    def __init__(self, model_name: str = "jhgan/ko-sroberta-multitask"):
        print("[VectorReranker] (ë¯¸ì‚¬ìš©) BM25 ìˆœì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        self.model = None  # ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì•ˆ í•¨

    def rerank(self, query: str, candidates: List[ChunkDocument], top_k: int = 5) -> List[ChunkDocument]:
        return candidates[:top_k]


class RAGPipeline:
    def __init__(self, bm25_top_k: int = 30, rerank_top_k: int = 5):
        # 1) BM25ìš© ì²­í¬ ë¡œë“œ
        self.chunk_docs = load_chunks_from_db()
        self.bm25 = BM25DBRetriever(self.chunk_docs)
        self.bm25_top_k = bm25_top_k
        self.rerank_top_k = rerank_top_k

        # id â†’ ChunkDocument ë§¤í•‘ (RRFì—ì„œ ì‚¬ìš©)
        self.id_to_doc: Dict[str, ChunkDocument] = {d.id: d for d in self.chunk_docs}

        # 2) ChromaDB ì—°ê²° (ë²¡í„° ê²€ìƒ‰ìš©)
        try:
            self.chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
            self.chroma_collection = self.chroma_client.get_or_create_collection(
                name=CHROMA_COLLECTION_NAME
            )
            print("[Chroma] ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ:", CHROMA_COLLECTION_NAME)
        except Exception as e:
            print(f"[Chroma] ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.chroma_client = None
            self.chroma_collection = None

        # 3) ì¿¼ë¦¬ ì„ë² ë”©ìš© SBERT 
        try:
            self.encoder = SentenceTransformer(EMBEDDING_MODEL_NAME)
            print(f"[Embedding] SentenceTransformer ë¡œë“œ ì™„ë£Œ: {EMBEDDING_MODEL_NAME}")
        except Exception as e:
            print(f"[Embedding] SentenceTransformer ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.encoder = None

    # -------------------------
    # 1) ë²¡í„° ê²€ìƒ‰ (Chroma)
    # -------------------------
    def vector_search(self, query: str, top_k: int = 30) -> List[ChunkDocument]:
        """
        ChromaDBì— ì €ì¥ëœ ì„ë² ë”© ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰
        """
        if self.chroma_collection is None or self.encoder is None:
            return []

        try:
            query_emb = self.encoder.encode([query])  # (1, embedding_dim)
            res = self.chroma_collection.query(
                query_embeddings=query_emb.tolist(),
                n_results=top_k,
                include=["metadatas"],   # ğŸ”¥ idsëŠ” ìµœì‹ ë²„ì „ì—ì„œ ì œê±°ë¨ 
            )
        except Exception as e:
            print(f"[Chroma] vector_search ì‹¤íŒ¨: {e}")
            return []

        metas = res.get("metadatas", [[]])[0]

        ids = []
        for m in metas:
            cid = m.get("new_id")  # vector_db.pyì—ì„œ ì €ì¥í•œ ì‹¤ì œ chunk ID
            if cid:
                ids.append(cid)

        docs: List[ChunkDocument] = []
        for cid in ids:
            doc = self.id_to_doc.get(cid)
            if doc:
                docs.append(doc)

        return docs

    # -------------------------
    # 2) RRF ê²°í•© í•¨ìˆ˜
    # -------------------------
    def _rrf_merge(
        self,
        bm25_docs: List[ChunkDocument],
        vec_docs: List[ChunkDocument],
        top_k: int = 30,
        k: int = 60,
    ) -> List[ChunkDocument]:
        """
        Reciprocal Rank Fusion:
        ê° ë­í¬ì— ëŒ€í•´ 1 / (k + rank)ë¥¼ ë”í•´ ì ìˆ˜ë¥¼ í•©ì‚°í•œ ë’¤,
        ìµœì¢… ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ top_kë¥¼ ë°˜í™˜í•œë‹¤.
        """
        scores: Dict[str, float] = {}

        # BM25 ìˆœìœ„
        for rank, d in enumerate(bm25_docs):
            scores[d.id] = scores.get(d.id, 0.0) + 1.0 / (k + rank + 1)

        # ë²¡í„° ê²€ìƒ‰ ìˆœìœ„
        for rank, d in enumerate(vec_docs):
            scores[d.id] = scores.get(d.id, 0.0) + 1.0 / (k + rank + 1)

        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_ids = sorted(scores.items(), key=lambda x: x[1], reverse=True)

        merged_docs: List[ChunkDocument] = []
        for cid, _score in sorted_ids:
            doc = self.id_to_doc.get(cid)
            if doc and doc not in merged_docs:
                merged_docs.append(doc)
            if len(merged_docs) >= top_k:
                break

        return merged_docs

    # -------------------------
    # 3) intent/slot í•„í„°
    # -------------------------
    def _apply_filters(
        self,
        candidates: List[ChunkDocument],
        intent: Optional[str],
        slots: Dict,
    ) -> List[ChunkDocument]:
        if intent:
            candidates = [
                d for d in candidates
                if (intent == "ê°•ì˜í‰_ê²€ìƒ‰" and d.meta.get("source") == "lecture_review")
                or (intent == "ê³µì§€_ê²€ìƒ‰" and d.meta.get("source") == "notice")
                or (intent == "ë™ì•„ë¦¬_ê²€ìƒ‰" and d.meta.get("source") == "club")
            ]

        prof = slots.get("professor_name") or slots.get("professor")
        if prof:
            candidates = [
                d for d in candidates
                if "professor" in d.meta and prof in d.meta["professor"]
            ]

        dept = slots.get("department")
        if dept:
            candidates = [
                d for d in candidates
                if "department" in d.meta and dept in d.meta["department"]
            ]

        club_name = slots.get("club_name")
        if club_name:
            candidates = [
                d for d in candidates
                if "club_name" in d.meta and club_name in d.meta["club_name"]
            ]

        return candidates

    # -------------------------
    # 4) ìµœì¢… Retrieve (BM25 + ë²¡í„° + RRF)
    # -------------------------
    def retrieve(self, query: str, intent: str = None, slots: Dict = None):
        slots = slots or {}

        # 1) BM25 í›„ë³´ ê²€ìƒ‰
        bm25_candidates = self.bm25.search(query, top_k=self.bm25_top_k)

        # 2) ë²¡í„° ê²€ìƒ‰ í›„ë³´
        vector_candidates = self.vector_search(query, top_k=self.bm25_top_k)

        # 3) RRFë¡œ ë‘ ê²°ê³¼ ê²°í•© (ë²¡í„° ê²€ìƒ‰ì´ ì‹¤íŒ¨í•˜ë©´ BM25ë§Œ ì‚¬ìš©)
        if vector_candidates:
            merged = self._rrf_merge(
                bm25_candidates,
                vector_candidates,
                top_k=self.bm25_top_k,
            )
        else:
            merged = bm25_candidates

        # 4) intent / slot í•„í„°
        candidates = self._apply_filters(merged, intent=intent, slots=slots)

        # 5) í•„í„° ê²°ê³¼ê°€ ë¹„ë©´ BM25 ì›ë³¸ìœ¼ë¡œ fallback
        if not candidates:
            candidates = bm25_candidates

        # 6) ìµœì¢… top_k ë°˜í™˜
        return candidates[:self.rerank_top_k]

    # -------------------------
    # 5) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    # -------------------------
    def build_prompt(self, query: str, docs: List[ChunkDocument]) -> tuple[str, str]:
        context_blocks = []
        for i, d in enumerate(docs, start=1):
            header = f"[ë¬¸ì„œ {i} | {d.meta.get('source', 'unknown')} | id={d.id}]"
            block = f"{header}\n{d.text}"
            context_blocks.append(block)
        context_text = "\n\n---\n\n".join(context_blocks)
        system_msg = (
            "ë„ˆëŠ” ìˆ­ì‹¤ëŒ€í•™êµ ê´€ë ¨ ì •ë³´ë§Œ ë‹µë³€í•˜ëŠ” ì±—ë´‡ì´ì•¼.\n"
            "ì•„ë˜ì— ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œë§Œ ê·¼ê±°ë¥¼ ì°¾ì•„ì„œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´.\n"
            "ëª¨ë¥´ê² ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•´."
        )
        user_msg = (
            f"ì‚¬ìš©ì ì§ˆë¬¸:\n{query}\n\n"
            f"ë‹¤ìŒì€ ê´€ë ¨ ë¬¸ì„œë“¤ì´ì•¼. ì´ ì •ë³´ë§Œ ê·¼ê±°ë¡œ ë‹µë³€ì„ ë§Œë“¤ì–´ì¤˜.\n\n"
            f"{context_text}"
        )
        return system_msg, user_msg

    # -------------------------
    # 6) LLM í˜¸ì¶œ ë˜í¼
    # -------------------------
    def answer_with_llm(
        self,
        query: str,
        llm_call: Callable[[str, str], str],
        intent: str = None,
        slots: Dict = None,
    ) -> str:
        """
        ì§ˆë¬¸/ì˜ë„ì— ë”°ë¼ í•™ì‹ ìŠ¤í¬ë˜í•‘ ë˜ëŠ” ì¼ë°˜ RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±.
        """

        # âœ… 1) ì§ˆë¬¸ ë¬¸ìì—´ë§Œ ë³´ê³  'í•™ì‹' ê´€ë ¨ ì˜ë„ ìë™ íŒë³„
        if (
            ("í•™ì‹" in query)
            or ("ë©”ë‰´" in query)
            or ("ë°¥ ë­" in query)
            or ("ë°¥ ë­ ë‚˜ì™€" in query)
            or ("ì˜¤ëŠ˜ ë°¥" in query)
            or ("ìƒí˜‘" in query)
            or ("ê¸°ìˆ™ì‚¬ ì‹ë‹¹" in query)
        ):
            intent = "í•™ì‹_ê²€ìƒ‰"

        # âœ… 2) í•™ì‹ ì˜ë„ë©´ RAG ë§ê³  ì‹¤ì‹œê°„ ìŠ¤í¬ë˜í•‘ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
        if intent == "í•™ì‹_ê²€ìƒ‰":
            meal_context = build_meal_context()
            system_msg = (
                "ë„ˆëŠ” ìˆ­ì‹¤ëŒ€í•™êµ í•™ì‹ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì´ì•¼.\n"
                "ì•„ë˜ ì»¨í…ìŠ¤íŠ¸(ìƒí˜‘/ê¸°ìˆ™ì‚¬ ì‹ë‹¹ ë©”ë‰´)ë¥¼ ì°¸ê³ í•´ì„œ, "
                "ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ê²Œ ì˜¤ëŠ˜ì˜ í•™ì‹ ì •ë³´ë¥¼ ê°„ëµí•˜ê³  ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•´ì„œ ì•Œë ¤ì¤˜.\n"
                "ë©”ë‰´ ì´ë¦„, ì½”ë„ˆ ì´ë¦„, ê°€ê²©, ë¼ë‹ˆ(ì¡°ì‹/ì¤‘ì‹/ì„ì‹) ë“±ì„ ì •ëˆí•´ì„œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´."
            )
            user_msg = (
                f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n"
                f"ë‹¤ìŒì€ ì˜¤ëŠ˜ì˜ í•™ì‹ ì •ë³´ì•¼:\n\n{meal_context}"
            )
            return llm_call(system_msg, user_msg)

        # âœ… 3) ê·¸ ì™¸ëŠ” ê¸°ì¡´ RAG íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        docs = self.retrieve(query, intent=intent, slots=slots or {})
        system_msg, user_msg = self.build_prompt(query, docs)
        return llm_call(system_msg, user_msg)

# ======================== Evalì„ ìœ„í•œ ì½”ë“œ ====================
# ragas, Recall@K ë“± ë‹¤ë¥¸ í‰ê°€ë¥¼ ìœ„í•œ ë¦¬í„´ íƒ€ì… ì •ì˜
def answer_with_llm_EVAL(
        self,
        query: str,
        llm_call: Callable[[str, str], str],
        intent: str = None,
        slots: Dict = None,
    ) -> EvaluationResult: # ğŸ’¡ ë¦¬í„´ íƒ€ì…: EvaluationResult
        """
        í‰ê°€ ì‘ì—…ì„ ìœ„í•´ ë‹µë³€ ì™¸ì— ê²€ìƒ‰ëœ ì²­í¬ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        (ê¸°ì¡´ answer_with_llmì˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, ë¦¬í„´ë§Œ ë³€ê²½)
        """
        slots = slots or {}

        # âœ… 1) ì§ˆë¬¸ ë¬¸ìì—´ë§Œ ë³´ê³  'í•™ì‹' ê´€ë ¨ ì˜ë„ ìë™ íŒë³„
        if (
            ("í•™ì‹" in query)
            or ("ë©”ë‰´" in query)
            or ("ë°¥ ë­" in query)
            or ("ë°¥ ë­ ë‚˜ì™€" in query)
            or ("ì˜¤ëŠ˜ ë°¥" in query)
            or ("ìƒí˜‘" in query)
            or ("ê¸°ìˆ™ì‚¬ ì‹ë‹¹" in query)
        ):
            intent = "í•™ì‹_ê²€ìƒ‰"

        # âœ… 2) í•™ì‹ ì˜ë„ë©´ RAG ë§ê³  ì‹¤ì‹œê°„ ìŠ¤í¬ë˜í•‘ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
        if intent == "í•™ì‹_ê²€ìƒ‰":
            meal_context = build_meal_context()
            system_msg = (
                "ë„ˆëŠ” ìˆ­ì‹¤ëŒ€í•™êµ í•™ì‹ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì´ì•¼.\n"
                "ì•„ë˜ ì»¨í…ìŠ¤íŠ¸(ìƒí˜‘/ê¸°ìˆ™ì‚¬ ì‹ë‹¹ ë©”ë‰´)ë¥¼ ì°¸ê³ í•´ì„œ, "
                "ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ê²Œ ì˜¤ëŠ˜ì˜ í•™ì‹ ì •ë³´ë¥¼ ê°„ëµí•˜ê³  ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•´ì„œ ì•Œë ¤ì¤˜.\n"
                "ë©”ë‰´ ì´ë¦„, ì½”ë„ˆ ì´ë¦„, ê°€ê²©, ë¼ë‹ˆ(ì¡°ì‹/ì¤‘ì‹/ì„ì‹) ë“±ì„ ì •ëˆí•´ì„œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´."
            )
            user_msg = (
                f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n"
                f"ë‹¤ìŒì€ ì˜¤ëŠ˜ì˜ í•™ì‹ ì •ë³´ì•¼:\n\n{meal_context}"
            )
            final_answer = llm_call(system_msg, user_msg)
            
            # ğŸ’¡ EvaluationResult ë°˜í™˜ (RAG flow ì•„ë‹˜)
            return EvaluationResult(
                query=query,
                model_answer=final_answer,
                retrieved_chunks=[],
                context_texts=[],
                is_rag_flow=False
            )

        # âœ… 3) ê·¸ ì™¸ëŠ” ê¸°ì¡´ RAG íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        
        # 1. ê²€ìƒ‰ ê²°ê³¼ í™•ë³´ (docs ë³€ìˆ˜ì— ì €ì¥)
        docs: List[ChunkDocument] = self.retrieve(query, intent=intent, slots=slots)
        
        # 2. LLM í˜¸ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_msg, user_msg = self.build_prompt(query, docs)
        
        # 3. LLM ë‹µë³€ ìƒì„±
        start_time = time.time()  # ğŸ’¡ ì¸¡ì • ì‹œì‘
        final_answer = llm_call(system_msg, user_msg)
        end_time = time.time()    # ğŸ’¡ ì¸¡ì • ì¢…ë£Œ

        latency = end_time - start_time # ğŸ’¡ ì†Œìš” ì‹œê°„ (ì´ˆ ë‹¨ìœ„)

        # 4. RAGAs 'contexts'ë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸ ëª©ë¡ ì¶”ì¶œ
        context_texts = [d.text for d in docs]
        
        # ğŸ’¡ ìµœì¢…ì ìœ¼ë¡œ EvaluationResult ê°ì²´ë¥¼ ë°˜í™˜
        return EvaluationResult(
            query=query,
            model_answer=final_answer,
            retrieved_chunks=docs,
            context_texts=context_texts,
            is_rag_flow=True,
            latency_seconds=latency # ğŸ’¡ ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€
        )



# =========================
# 6. GPT API í˜¸ì¶œ í•¨ìˆ˜
# =========================

def call_openai_api(system_msg: str, user_msg: str) -> str:
    """
    OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    .env íŒŒì¼ì—ì„œ í‚¤ë¥¼ ë¡œë“œí•˜ë¯€ë¡œ ë³´ì•ˆìƒ ì•ˆì „í•©ë‹ˆë‹¤.
    """
    api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        return "[ì˜¤ë¥˜] .env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

    try:
        client = OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.0,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"[LLM í˜¸ì¶œ ì˜¤ë¥˜] API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"





# =========================
# 8. ê°„ë‹¨ CLI í…ŒìŠ¤íŠ¸ìš© main
# =========================

if __name__ == "__main__":
    print(f"[RAG] Using DB_PATH = {DB_PATH}")
    print("í„°ë¯¸ë„ì—ì„œ ì§ì ‘ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. 'í•™ì‹'ì´ë¼ê³  ì³ë³´ì„¸ìš”.\n")

    while True:
        try:
            q = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: ì—”í„°ë§Œ ì…ë ¥): ").strip()
        except (EOFError, KeyboardInterrupt):
            break

        if not q:
            break

        print("\n--- ğŸ§  LLMì´ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... ---")

        # ì—¬ê¸°ì„œ intentëŠ” êµ³ì´ ì•ˆ ì¤˜ë„ ë˜ì§€ë§Œ, ë„£ì–´ë„ ìƒê´€ ì—†ìŒ
        if ("í•™ì‹" in q) or ("ë©”ë‰´" in q) or ("ë°¥ ë­" in q):
            intent = "í•™ì‹_ê²€ìƒ‰"
        else:
            intent = None

        answer = rag.answer_with_llm(q, llm_call=call_openai_api, intent=intent)

        print("\n=======================================================")
        print(f"[ê¶ê¸ˆí–ˆìŠˆ(SSU) ë‹µë³€]\n{answer}")
        print("=======================================================\n")
import json
import numpy as np

# ======================
# ì´ ì„¸ ê°€ì§€ ì§€í‘œëŠ” ì±—ë´‡ì´ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µ ë¬¸ì„œë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê³  
# ë¹ ë¥´ê²Œ ìƒìœ„ ìˆœìœ„ì— ë°°ì¹˜í–ˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤.
# 1. Recall@KëŠ” ì‹œìŠ¤í…œì´ ì œì‹œí•œ ìƒìœ„ Kê°œì˜ ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ ì•ˆì— ì •ë‹µ í•­ëª©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ 
# ì—¬ë¶€ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. 
# 2. MMR(Mean Reciprocal Rank, í‰ê·  ì—­ìˆœìœ„)ì€ ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì—ì„œ 
# ì²« ë²ˆì§¸ë¡œ ë‚˜íƒ€ë‚œ ì •ë‹µ í•­ëª©ì˜ ìˆœìœ„ê°€ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. 
# MRRì€ ì •ë‹µì„ 1ìœ„ì— ë°°ì¹˜í•˜ëŠ” ì‹œìŠ¤í…œ(1.0ì  íšë“)ì´ 5ìœ„(0.2ì  íšë“)ì— ë°°ì¹˜í•˜ëŠ” ì‹œìŠ¤í…œë³´ë‹¤ í›¨ì”¬ ìš°ìˆ˜í•˜ë‹¤ê³  í‰ê°€í•©ë‹ˆë‹¤. 
# 3. RRF (Reciprocal Rank Fusion, ì—­ìˆœìœ„ ìœµí•©)ëŠ” 
# ì—¬ëŸ¬ ë…ë¦½ì ì¸ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ìˆœìœ„ ëª©ë¡ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ 
# ìµœì¢… ìˆœìœ„ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì ìˆ˜ ì²´ê³„ì…ë‹ˆë‹¤. 
# ì‘ì—…ì : ë°•ì±„ì€

def calculate_retrieval_metrics_with_latency(results_path: str, K: int = 5):
    """
    ì €ì¥ëœ JSON ê²°ê³¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ Recall@K, MRR, RRF ë° í‰ê·  ë ˆì´í„´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    with open(results_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ì§€í‘œ ê³„ì‚°ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    recall_scores = []
    mrr_scores = []
    rrf_scores = [] 
    latency_scores = [] # ğŸ’¡ ë ˆì´í„´ì‹œ ì¸¡ì •ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
    
    # RRF ê³„ì‚°ì„ ìœ„í•œ ìƒìˆ˜
    k_rrf = 60 

    for item in data:
        # 1. í•„ìˆ˜ ë°ì´í„° ì¶”ì¶œ
        gt_id = item.get('ground_truth_chunk_id')
        retrieved_list = item.get('retrieved_chunks', [])
        
        # ğŸ’¡ ë ˆì´í„´ì‹œ ê°’ ì¶”ì¶œ (RAG flowê°€ ì•„ë‹Œ ê²½ìš°ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬)
        latency = item.get('latency_seconds', 0.0)
        
        # RAG flowë¥¼ íƒ„ ê²½ìš°ì—ë§Œ ê²€ìƒ‰ ì§€í‘œ ë° ë ˆì´í„´ì‹œë¥¼ ì¸¡ì • ëŒ€ìƒìœ¼ë¡œ ì‚¼ìŒ
        if item.get('is_rag_flow', False):
            latency_scores.append(latency) # ğŸ’¡ ë ˆì´í„´ì‹œ ì ìˆ˜ ëˆ„ì 

            # 2. ì •ë‹µ ì²­í¬ì˜ ìˆœìœ„(Rank) ì°¾ê¸°
            found_rank = 0
            for index, chunk in enumerate(retrieved_list):
                if chunk.get('id') == gt_id:
                    found_rank = index + 1
                    break
            
            # 3. ê²€ìƒ‰ ì§€í‘œ ê³„ì‚°
            recall_scores.append(1 if (found_rank > 0 and found_rank <= K) else 0)
            mrr_scores.append(1.0 / found_rank if found_rank > 0 else 0.0)
            rrf_scores.append(1.0 / (k_rrf + found_rank) if found_rank > 0 else 0.0)


    # 4. ìµœì¢… í‰ê·  ê³„ì‚° ë° ê²°ê³¼ í†µí•©
    metrics = {
        # ğŸ’¡ ì „ì²´ í‰ê·  ë ˆì´í„´ì‹œ ì¶”ê°€
        "Average Latency (sec)": np.mean(latency_scores) if latency_scores else 0.0,
        
        # ê²€ìƒ‰ ì •í™•ë„ ì§€í‘œ
        f"Mean Recall@{K}": np.mean(recall_scores) if recall_scores else 0.0,
        "MRR (Mean Reciprocal Rank)": np.mean(mrr_scores) if mrr_scores else 0.0,
        f"Mean RRF (k={k_rrf})": np.mean(rrf_scores) if rrf_scores else 0.0
    }
    
    return metrics

# ------------------------------------------------------------------
# ì‚¬ìš© ì˜ˆì‹œ
# ------------------------------------------------------------------
results_file = "Evaluation/data/rag_evaluation_results_full.json"
retrieval_metrics = calculate_retrieval_metrics_with_latency(results_file, K=5)
print(retrieval_metrics)
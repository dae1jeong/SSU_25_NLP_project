# ==============================================================================
# SSU_25_NLP_project - data/generate_dataset.py (v1.1 - RAG ì¼ê´€ì„± í™•ë³´)
#
# [ë³€ê²½ ëª©ì ]
#
# í•©ì„± ë°ì´í„°ì…‹ ìƒì„± ì‹œ ì»¨í…ìŠ¤íŠ¸(Context)ë¥¼ ì‹¤ì œ RAG ì±—ë´‡ì˜ ê²€ìƒ‰ ê²°ê³¼ì™€ ë™ì¼í•˜ê²Œ êµ¬ì„±í•˜ì—¬,
# RAG í‰ê°€ ì§€í‘œì˜ **ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±(Realism)**ì„ í™•ë³´í•©ë‹ˆë‹¤.
# vector_db.pyì˜ ì²­í‚¹ ì „ëµ(400ì ì²­í¬, 50ì ê²¹ì¹¨)ê³¼ ì™„ì „íˆ ì¼ì¹˜ì‹œí‚µë‹ˆë‹¤.
#
# [ì£¼ìš” ë³€ê²½ ì‚¬í•­]
#
# ì²­í‚¹ ë¡œì§ í†µí•©: generate_dataset.pyì˜ load_all_data í•¨ìˆ˜ ë‚´ì—ì„œ vector_db.pyì™€
# ë™ì¼í•œ ì„¤ì •ì˜ **RecursiveCharacterTextSplitter**ë¥¼ ì‚¬ìš©í•˜ì—¬ DBì—ì„œ ë¡œë“œëœ ì›ë¬¸ì— ì²­í‚¹ì„ ì ìš©í•©ë‹ˆë‹¤.
#
# ë°ì´í„° ì†ŒìŠ¤ë³„ ì²˜ë¦¬:
#
# ê°•ì˜í‰ (lecture_reviews): ì›ë¬¸ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì²­í‚¹ ì œì™¸).
#
# ê³µì§€ì‚¬í•­ (notices) ë° ë™ì•„ë¦¬ (clubs): ê¸´ ì›ë¬¸ì„ CHUNK_SIZE=400, CHUNK_OVERLAP=50ì„ ì‚¬ìš©í•˜ì—¬
# ì²­í‚¹í•œ í›„, ê° ì²­í¬ë¥¼ ë…ë¦½ì ì¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ QA ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
#
# [íš¨ê³¼]
#
# í•©ì„± ë°ì´í„°ì…‹ì˜ contexts í•„ë“œì— ë‹´ê¸°ëŠ” ë¬¸ì„œ ì¡°ê°ë“¤ì´ ì‹¤ì œ ì±—ë´‡ì´ ê²€ìƒ‰í•˜ì—¬ LLMì— ì „ë‹¬í•˜ëŠ” ì¡°ê°ê³¼ ì¼ì¹˜í•˜ë¯€ë¡œ,
# ìƒì„±ëœ QA ìŒì´ RAG ì‹œìŠ¤í…œì˜ ì‹¤ì œ ë™ì‘ì„ ë” ì •í™•í•˜ê²Œ ë°˜ì˜í•©ë‹ˆë‹¤.
# ==============================================================================

# ==============================================================================
# chorma_db.pyë¥¼ ì‹¤í–‰ ì‹œ ìƒê¸´ ì¤‘ê°„ ê²°ê³¼ë¬¼ jsonlì„ ì‚¬ìš©í•´ generate dataset.í•¨.
#
# ==============================================================================





import os
import json
import sqlite3
import random
from tqdm import tqdm
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI


# vector_db.pyê°€ ìƒì„±í•œ íŒŒì¼ëª…ê³¼ ì¼ì¹˜ì‹œì¼œì•¼ í•¨
CHUNKED_DATA_PATH = "chunked_data.jsonl" 



# ---------------------------
# í™˜ê²½ ì„¤ì •
# ---------------------------
load_dotenv()
DB_PATH = "ssu_chatbot_data.db"
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

CHUNK_SIZE = 400
CHUNK_OVERLAP = 50
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    separators=["\n\n", "\n", ".", "!", "?", " "]
)

# ---------------------------
# DBì—ì„œ ë°ì´í„° ë¡œë“œ
# ---------------------------
def load_all_data():
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    lecture_reviews = []
    notices_chunks = []
    clubs_chunks = []

    # --- lecture_reviews (ì²­í‚¹ X)
    cursor.execute("SELECT id, subject_name, professor_name, review_text, semester FROM lecture_reviews")
    for row in cursor.fetchall():
        text = f"ê³¼ëª©ëª…: {row[1]}, êµìˆ˜ëª…: {row[2]}, ê°•ì˜í‰: {row[3]}"
        lecture_reviews.append(text)

    # --- notices (ì²­í‚¹ O)
    cursor.execute("SELECT id, title, category, full_body_text FROM notices")
    for row in cursor.fetchall():
        chunks = text_splitter.split_text(row[3])
        for chunk in chunks:
            text = f"ê³µì§€: {row[1]} (ì¹´í…Œê³ ë¦¬: {row[2]})\në‚´ìš©: {chunk}"
            notices_chunks.append(text)

    # --- clubs (ì²­í‚¹ O)
    cursor.execute("SELECT id, club_name, category, description FROM clubs")
    for row in cursor.fetchall():
        chunks = text_splitter.split_text(row[3])
        for chunk in chunks:
            text = f"ë™ì•„ë¦¬: {row[1]} (ë¶„ê³¼: {row[2]})\nì†Œê°œ: {chunk}"
            clubs_chunks.append(text)

    conn.close()
    return lecture_reviews, notices_chunks, clubs_chunks

# ---------------------------
# ì²­í‚¹ëœ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ---------------------------
# DB ë¡œë“œ í•¨ìˆ˜ ë§ê³  ì´ê±¸ ì‚¬ìš©í•˜ë„ë¡ mainì—ì„œ ì½”ë“œ ìˆ˜ì • í•„ìš”
def load_all_data_from_chunks():
    """
    DBì—ì„œ ì§ì ‘ ë¡œë“œ ë° ì²­í‚¹í•˜ëŠ” ëŒ€ì‹ , vector_db.pyê°€ ìƒì„±í•œ JSONL íŒŒì¼ì—ì„œ ì²­í‚¹ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    lecture_reviews = []
    notices_chunks = []
    clubs_chunks = []

    if not os.path.exists(CHUNKED_DATA_PATH):
        print(f"ì˜¤ë¥˜: ì²­í‚¹ ê²°ê³¼ íŒŒì¼ '{CHUNKED_DATA_PATH}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. vector_db.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return lecture_reviews, notices_chunks, clubs_chunks

    print(f"-> ì²­í‚¹ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘: {CHUNKED_DATA_PATH}")
    with open(CHUNKED_DATA_PATH, 'r', encoding='utf-8') as f:
        for line in tqdm(f, desc="ì²­í¬ ë°ì´í„° ë¡œë“œ"):
            item = json.loads(line.strip())
            text = item['text']
            source = item['metadata']['source']
            chunk_id = item['id']  # ìˆ˜ì •ëœ ë¶€ë¶„....... text, chunk_idë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ë„ë¡.

            if source == "lecture_review":
                lecture_reviews.append((text, chunk_id))
            elif source == "notice":
                notices_chunks.append((text, chunk_id))
            elif source == "club":
                clubs_chunks.append((text, chunk_id))
    
    return lecture_reviews, notices_chunks, clubs_chunks


# ---------------------------
# ëœë¤ ìƒ˜í”Œë§ (ë¹„ìœ¨ ë§ì¶¤)
# ---------------------------
def sample_documents(lecture_reviews, notices_chunks, clubs_chunks, NUM_QA=10, ratios=(2,5,3)):
    total_ratio = sum(ratios)
    num_lecture = round(NUM_QA * ratios[0] / total_ratio)
    num_notice = round(NUM_QA * ratios[1] / total_ratio)
    num_club = NUM_QA - num_lecture - num_notice

    sampled_docs = []
    if lecture_reviews:
        sampled_docs += random.sample(lecture_reviews, min(num_lecture, len(lecture_reviews)))
    if notices_chunks:
        sampled_docs += random.sample(notices_chunks, min(num_notice, len(notices_chunks)))
    if clubs_chunks:
        sampled_docs += random.sample(clubs_chunks, min(num_club, len(clubs_chunks)))

    random.shuffle(sampled_docs)
    return sampled_docs

# ---------------------------
# QA ìƒì„±
# ---------------------------
def generate_qa_pair(text):
    prompt = f"""
    ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ì½ê³ , ì±—ë´‡ ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¼ ë§Œí•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ì •ë‹µ 1ê°œë¥¼ ìƒì„±í•´ì¤˜.
    ì •ë‹µì€ ë°˜ë“œì‹œ ì œê³µëœ í…ìŠ¤íŠ¸ì— ìˆëŠ” ë‚´ìš©ë§Œ ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤.

    [í…ìŠ¤íŠ¸]:
    {text[:1000]}

    [ì¶œë ¥ JSON]:
    {{
        "question": "ìƒì„±ëœ ì§ˆë¬¸",
        "ground_truth": "í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì •ë‹µ"
    }}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ë°ì´í„°ì…‹ ìƒì„±ê¸°ì•¼. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ëŒ€ë‹µí•´."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.7  # ì°½ì˜ì ì¸ ì •ë„
        )
        return json.loads(response.choices[0].message.content)
    except Exception:
        return None

# ---------------------------
# ë©”ì¸ ì‹¤í–‰
# ---------------------------
if __name__ == "__main__":
    NUM_QA = 10    #ì§ˆë¬¸ ìˆ˜ ì €ì¥
    lecture_reviews, notices_chunks, clubs_chunks = load_all_data_from_chunks() # load_all_data_from_chunks()
    sampled_docs = sample_documents(lecture_reviews, notices_chunks, clubs_chunks, NUM_QA=NUM_QA, ratios=(2,5,3))

    dataset = []
    for doc_tuple in tqdm(sampled_docs, desc="QA ìƒì„±ì¤‘"):
        text, chunk_id = doc_tuple # ğŸ’¡ ìˆ˜ì • 2: íŠœí”Œì—ì„œ textì™€ chunk_idë¥¼ ë¶„ë¦¬
        qa = generate_qa_pair(text)
        if qa:
            # ğŸ’¡ ìˆ˜ì • 3: ìƒì„±ëœ QA ìŒì— chunk_idë¥¼ ì¶”ê°€
            qa['ground_truth_chunk_id'] = chunk_id
            dataset.append(qa)

    os.makedirs("Evaluation/data", exist_ok=True)
    output_file = "Evaluation/data/ragas_qa_dataset2.jsonl"  #OUTPUT ê²½ë¡œ
    with open(output_file, "w", encoding="utf-8") as f:
        for item in dataset:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"QA ìƒì„± ì™„ë£Œ! ì´ {len(dataset)}ê°œ ì €ì¥ë¨ â†’ {output_file}")

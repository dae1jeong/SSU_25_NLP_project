#  rag_test_results_1129_*
# 1~11ê¹Œì§€ blue score print
import json
import sacrebleu
import os
from tqdm import tqdm
from typing import List, Dict, Any, Tuple

# íŒŒì¼ ê²½ë¡œ ì„¤ì • (í˜„ì¬ ìƒí™©ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”)
BASE_PATH = "Evaluation/data/"
FILE_PREFIX = "rag_test_results_1129_"
NUM_FILES = 11

def _load_data_and_extract(file_path: str) -> Tuple[List[str], List[List[str]]]:
    """ë‹¨ì¼ JSON íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ëª¨ë¸ ë‹µë³€ê³¼ ì •ë‹µì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    if not os.path.exists(file_path):
        return [], []
    
    hypotheses = []
    references = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
            for item in data:
                model_answer = item.get('model_answer')
                ground_truth = item.get('ground_truth')
                
                if model_answer and ground_truth:
                    hypotheses.append(model_answer)
                    # SacreBLEUëŠ” ì°¸ì¡° ë¬¸ì¥ì„ List[List[str]] í˜•íƒœë¡œ ìš”êµ¬í•©ë‹ˆë‹¤.
                    references.append([ground_truth]) 
        except json.JSONDecodeError:
            print(f"âŒ ì˜¤ë¥˜: {file_path} íŒŒì¼ JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ.")
            return [], []
            
    return hypotheses, references

def calculate_all_files_individually():
    """1ë²ˆë¶€í„° NUM_FILESê¹Œì§€ ìˆœíšŒí•˜ë©° íŒŒì¼ë³„ ì½”í¼ìŠ¤ BLEU ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    
    individual_scores = {}
    
    print(f"ğŸ“„ ì´ {NUM_FILES}ê°œ íŒŒì¼ì˜ BLEU ì ìˆ˜ë¥¼ ê°œë³„ ê³„ì‚°í•©ë‹ˆë‹¤.")

    for i in tqdm(range(1, NUM_FILES + 1), desc="íŒŒì¼ë³„ BLEU ê³„ì‚° ì¤‘"):
        file_name = f"{FILE_PREFIX}{i}.json"
        file_path = os.path.join(BASE_PATH, file_name)
        
        hypotheses, references = _load_data_and_extract(file_path)
        total_samples = len(hypotheses)
        
        if total_samples == 0:
            individual_scores[file_name] = {"BLEU-4 Score": 0.0, "Samples": 0}
            continue

        # SacreBLEU ì½”í¼ìŠ¤ BLEU ê³„ì‚°
        # list(zip(*references))ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í˜•íƒœë¥¼ ë§ì¶¥ë‹ˆë‹¤.
        bleu = sacrebleu.corpus_bleu(hypotheses, list(zip(*references)))
        
        individual_scores[file_name] = {
            "BLEU-4 Score": round(bleu.score, 4), 
            "Samples": total_samples
        }
        
    return individual_scores

# ==============================================================================
# ğŸš€ ì‹¤í–‰ ë¶€ë¶„
# ==============================================================================
if __name__ == "__main__":
    
    # 1. íŒŒì¼ë³„ BLEU ì ìˆ˜ ê³„ì‚°
    scores = calculate_all_files_individually()
    
    # 2. ê²°ê³¼ ì¶œë ¥ (ìš”ì²­í•˜ì‹  íŒŒì¼ëª… / ìŠ¤ì½”ì–´ í˜•íƒœ)
    print("\n" + "="*50)
    print("         ğŸ“‹ ê°œë³„ íŒŒì¼ ì½”í¼ìŠ¤ BLEU í‰ê°€ ê²°ê³¼ ğŸ“‹           ")
    print("="*50)
    print(f"{'íŒŒì¼ëª…':<30} | {'BLEU-4 Score':<15} | {'Samples':<8}")
    print("-" * 50)
    
    # ê²°ê³¼ë¥¼ íŒŒì¼ëª… ìˆœì„œëŒ€ë¡œ ì¶œë ¥
    for filename, result in sorted(scores.items()):
        score_str = f"{result['BLEU-4 Score']:.4f}"
        print(f"{filename:<30} | {score_str:<15} | {result['Samples']:<8}")
        
    print("="*50)

#          ğŸ“‹ ìµœì¢… BLEU-4 í‰ê°€ ê²°ê³¼ ğŸ“‹
# ==============================================
# Total Samples       : 1100
# BLEU-4 Score        : 6.0441
# SacreBLEU Details   : BLEU = 6.04 13.8/6.9/4.5/3.1 (BP = 1.000 ratio = 2.792 hyp_len = 39923 ref_len = 14297)
# ==============================================


# ==================================================
#          ğŸ“‹ ê°œë³„ íŒŒì¼ ì½”í¼ìŠ¤ BLEU í‰ê°€ ê²°ê³¼ ğŸ“‹
# ==================================================
# íŒŒì¼ëª…                            | BLEU-4 Score    | Samples
# --------------------------------------------------
# rag_test_results_1129_1.json   | 6.3479          | 100
# rag_test_results_1129_10.json  | 6.4141          | 100
# rag_test_results_1129_11.json  | 5.4387          | 100
# rag_test_results_1129_2.json   | 4.2161          | 100
# rag_test_results_1129_3.json   | 4.9771          | 100
# rag_test_results_1129_4.json   | 0.0638          | 100
# rag_test_results_1129_5.json   | 22.5835         | 100
# rag_test_results_1129_6.json   | 19.2541         | 100
# rag_test_results_1129_7.json   | 4.9970          | 100
# rag_test_results_1129_8.json   | 3.5341          | 100
# rag_test_results_1129_9.json   | 3.7315          | 100
# ==================================================
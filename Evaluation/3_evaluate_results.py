import json
import os
from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm

# .env ë¡œë“œ
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ".env"))
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def evaluate_answer(question, ground_truth, predicted):
    """GPT-4oë¥¼ ì´ìš©í•œ LLM-as-a-Judge í‰ê°€"""
    
    prompt = f"""
    ë„ˆëŠ” ê³µì •í•˜ê³  ì—„ê²©í•œ ì±„ì ê´€ì´ì•¼. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ AI ì±—ë´‡ì˜ ë‹µë³€ í’ˆì§ˆì„ 1ì ì—ì„œ 5ì  ì‚¬ì´ë¡œ í‰ê°€í•´ì¤˜.
    
    [ì§ˆë¬¸]: {question}
    [ì •ë‹µ(ê¸°ì¤€)]: {ground_truth}
    [ì±—ë´‡ ë‹µë³€]: {predicted}
    
    [í‰ê°€ ê¸°ì¤€]
    5ì : ì •ë‹µì˜ í•µì‹¬ ë‚´ìš©ì„ ë¹ ì§ì—†ì´ í¬í•¨í•˜ë©°, ì„¤ëª…ì´ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ì›€.
    4ì : í•µì‹¬ ë‚´ìš©ì€ í¬í•¨í–ˆìœ¼ë‚˜, ì‚¬ì†Œí•œ ì •ë³´ê°€ ëˆ„ë½ë˜ê±°ë‚˜ ì•½ê°„ ë¶€ìì—°ìŠ¤ëŸ¬ì›€.
    3ì : ì •ë‹µì˜ ì¼ë¶€ë§Œ ë§ê±°ë‚˜, ë¶ˆí•„ìš”í•œ ì •ë³´ê°€ ì„ì—¬ ìˆì–´ ëª…í™•í•˜ì§€ ì•ŠìŒ.
    2ì : ì§ˆë¬¸ê³¼ ê´€ë ¨ì€ ìˆìœ¼ë‚˜ í•µì‹¬ ì •ë³´ê°€ í‹€ë ¸ê±°ë‚˜ ì—‰ëš±í•œ ëŒ€ë‹µì„ í•¨.
    1ì : ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆê±°ë‚˜ ì™„ì „íˆ í‹€ë¦° ì •ë³´ë¥¼ ì œê³µí•¨.
    
    [ì¶œë ¥ í˜•ì‹ (JSON)]:
    {{
        "score": ì ìˆ˜(1~5 ì •ìˆ˜),
        "reason": "ì ìˆ˜ë¥¼ ë¶€ì—¬í•œ ì´ìœ  (í•œ ë¬¸ì¥ìœ¼ë¡œ)"
    }}
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",  # âœ… ì±„ì ì€ ë˜‘ë˜‘í•œ 4o ì‚¬ìš©!
            messages=[{"role": "system", "content": "ë„ˆëŠ” ì±„ì ê´€ì´ì•¼. JSONìœ¼ë¡œë§Œ ë‹µí•´."},
                      {"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except Exception:
        return {"score": 0, "reason": "ì±„ì  ì¤‘ ì—ëŸ¬ ë°œìƒ"}

def main():
    input_path = "Evaluation/rag_test_results.json"
    if not os.path.exists(input_path):
        print("âŒ ì‹¤í–‰ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 2ë²ˆ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    with open(input_path, "r", encoding="utf-8") as f:
        results = json.load(f)
        
    print(f"âš–ï¸ GPT-4o ì±„ì ê´€ì´ {len(results)}ê°œì˜ ë‹µì•ˆì„ ì±„ì í•©ë‹ˆë‹¤...")
    
    total_score = 0
    evaluated_results = []
    
    for item in tqdm(results):
        eval_result = evaluate_answer(item['question'], item['ground_truth'], item['predicted_answer'])
        
        item['score'] = eval_result['score']
        item['reason'] = eval_result['reason']
        
        total_score += item['score']
        evaluated_results.append(item)
        
    if len(results) > 0:
        avg_score = total_score / len(results)
    else:
        avg_score = 0
        
    print(f"\nğŸ“Š [ìµœì¢… ì„±ì í‘œ]")
    print(f"   - ì´ ë¬¸ì œ ìˆ˜: {len(results)}ê°œ")
    print(f"   - í‰ê·  ì ìˆ˜: {avg_score:.2f} / 5.0 ì ")
    
    # ê²°ê³¼ ì €ì¥
    with open("Evaluation/data/final_evaluation_report.json", "w", encoding="utf-8") as f:
        json.dump(evaluated_results, f, ensure_ascii=False, indent=2)
        
    print("âœ… ì±„ì  ì™„ë£Œ! ìƒì„¸ ê²°ê³¼: Evaluation/data/final_evaluation_report.json")

if __name__ == "__main__":
    main()